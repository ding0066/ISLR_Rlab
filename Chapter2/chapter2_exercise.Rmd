### Exercise 1 ###

(a) better - a more flexible approach will fit the data closer and with the large sample size a better fit than an inflexible approach would be obtained

(b) worse - a flexible method would over-fit the small number of observations

(c) better - a more flexible approach will be able to better fit the non-linear relationship between the predictors and the response

(d) worse - flexible method fit the noise as well which increase the variance

### Exercise 2 ###

(a) regression problem, we are more interested in inference

n: 500 firms in the US
p: profit, number of employees, industry

(b) classification problem, we are more interested in prediction

n: 20 similar products that were previously launched
p: price charged, marketing budget, competition price, and ten other variables

(c) regression problem, we are more interested in prediction

n: 52 weeks of 2012 weekly data
p: % change in the US market, % change in the British market, the % change in the German market

### Exercise 3 ###

(b) 

Training MSE reduced as more flexible methods introduced since more flexible methods tend to better reduce training MSE

Test MSE tend to reach minimum level before more flexible methods overfit the model

Bayes (or irreducible) error should be a straight line slightly lower than minimum point of Test MSE representing the error that cannot be reduced by choosing a different fitting method

Variance tend to increase as method become more and more flexible

Bias tend to decrease as method become more and more flexible

### Exercise 4 ###

(a) 

Decide whether to buy or sell a bond
Decide whether to promote or not promote an employee
Analyses whether a new album will hit the market well

(b)

Predict stock price level
Allocate marketing budget based on past performance
Attribute portfolio return to factors

(c) 

Classify emails into relevant categories
Classify baseball players type
Understand DNA functionality

### Exercise 5 ###

The advantages for a very flexible approach for regression or classification are obtaining a better fit for non-linear models, decreasing bias

The disadvantages for a very flexible approach for regression or classification are requires estimating a greater number of parameters, follow the noise too closely, increasing variance

A more flexible approach would be preferred to a less flexible approach when we are interested in prediction and not the interpret-ability of the results

A less flexible approach would be preferred to a more flexible approach when we are interested in inference and the interpret-ability of the results

### Exercise 6 ###

A parametric approach reduces the problem of estimating f down to one of estimating a set of parameters because it assumes a form for f

A non-parametric approach does not assume a functional form for f and so requires a very large number of observations to accurately estimate f

The advantages of a parametric approach to regression or classification are the simplifying of modeling f to a few parameters and not as many observations are required compared to a non-parametric approach

The disadvantages of a parametric approach to regression or classification are a potential to inaccurately estimate f if the form of f assumed is wrong or to overfit the observations if more flexible models are used

### Exercise 7 ###

(a) 

3
2
sqrt(10) = 3.2
sqrt(5) = 2.2
sqrt(2) = 1.4
sqrt(3) = 1.7

(b) Green

(c) Red

(d) Small, a small K would be flexible for a non-linear decision boundary, whereas a large K would try to fit a more linear boundary because it takes more points into consideration

### Exercise 8 ###

(a) Use the read.csv() function to read the data into R. Call the loaded data college.

```{r}
college <- read.csv("College.csv")
```

(b) Look at the data using the fix() function. Convert first column of university name to row name and remove it from the data set.

```{r}
rownames(college) <- college[,1]

college <- college[,-1]
```

(c)

i. Use the summary() function to produce a numerical summary of the variables in the data set

```{r}
summary(college)
```

ii. Use the pairs() function to produce a scatterplot matrix of the first ten columns or variables of the data.

```{r}
pairs(college[,1:10])
```

iii. Use the plot() function to produce side-by-side boxplots of Outstate versus Private.

```{r}
college$Private <- as.factor(college$Private)
plot(x = college$Private, y = college$Outstate)
```

iv. Create a new qualitative variable, called Elite, by binning the Top10Perc variable.

```{r}
Elite <- rep("No",nrow(college))
Elite[college$Top10perc > 50] <- "Yes"
Elite <- as.factor(Elite)
college <- data.frame(college, Elite)
```

```{r}
summary(college)
```

```{r}
plot(x = college$Elite, y = college$Outstate)
```

v. Use the hist() function to produce some histograms with differing numbers of bins for a few of the quantitative variables. par(mfrow=c(2,2)) is useful to simutaneously plot several histograms

```{r}
par(mfrow=c(2,2))
hist(college$Outstate,breaks = 20)
hist(college$Books,breaks = 20)
hist(college$Personal,breaks = 20)
hist(college$Grad.Rate,breaks = 20)
```

### Exercise 9 ###

(a) Which of the predictors are quantitative, and which are qualitative?

```{r}
auto <- read.csv("Auto.csv")
auto$origin <- as.factor(auto$origin)
auto$horsepower <- as.numeric(auto$horsepower)
summary(auto)
```

(b) What is the range of each quantitative predictor?

```{r}
sapply(auto[, 1:7], range)
```

(c) What is the mean and standard deviation of each quantitative predictor

```{r}
sapply(auto[, 1:7], mean)
sapply(auto[, 1:7], sd)
```

(d) Now remove the 10th through 85th observations, what is the range, mean, and standard deviation of each predictor in the subset of the data that remains?

```{r}
sapply(auto[-c(10:85),1:7],range)
sapply(auto[-c(10:85),1:7],mean)
sapply(auto[-c(10:85),1:7],sd)
```

(e) Using the full data set, investigate the predictrs graphically, create some plots highlighting the relationships among the predictors.
(f) Suppose that we wish to predict gas mimleage on the basis of the other variables. Do your plots suggest that any of the other variables might be useful in predicting mpg?

```{r}
plot(auto$displacement, auto$mpg)
plot(auto$weight, auto$mpg)
plot(auto$year, auto$mpg)
plot(auto$origin, auto$mpg)
```

mpg is negatively correlated with displacement and weight. It is positively correlated with years and origin.

### Exercise 10 ###

(a) To begin, load in the Boston data set. The Boston data set is part of the MASS library in R.

```{r}
library(MASS)
Boston
```

(b) Make some pairwise scatterplots of the predictors (columns) in this data set. Describe your findings.

```{r}
pairs(Boston[,1:10])
```

(c) Are any of the predictors associated with per capita crime rate? If so explain the relationship.

```{r}
plot(Boston$dis, Boston$crim)
```

Distance seems to be a huge correlation factor. the closer it is to one of the five Boston employment centres, the higher the crime rate.

(d) Does any of the suburbs of Boston appear to have particularly high crime rates? Tax rates? Pupil-teacher ratios?

```{r}
par(mfrow=c(1,3))
hist(Boston$crim, breaks = 25)
hist(Boston$tax, breaks = 25)
hist(Boston$ptratio, breaks = 25)
```

(e) How many of the suburbs in this data set bound the Charles river

```{r}
dim(subset(Boston, chas == 1))
```

(f) What is the median pupil-teacher ratio among the tows in this data set?

```{r}
median(Boston$ptratio)
```

(g) Which suburb of Boston has lowest median value of owner-occupied homes?

```{r}
t(subset(Boston, medv == min(Boston$medv)))
```

(h) In this data set, how many of the suburbs average more than seven rooms per dwelling? More than eight rooms per dwelling?

```{r}
dim(subset(Boston, rm > 7))
```

```{r}
subset(Boston, rm > 8)
```

