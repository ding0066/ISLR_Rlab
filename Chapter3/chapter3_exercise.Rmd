---
output:
  pdf_document: default
  html_document: default
---
### Question 1 ###

Based on Table 3.4, we can conclude that both TV and radio is has significant impact on sales performance. While newspaper does not prove to have a significant impact on sales.

### Question 2 ###

KNN regression averages the closest observations to estimate predict, KNN classifier assigns classification group based on majority of closest observations.

### Question 3 ###

(a)

i. inccorect, since there is an interaction term between gender and GPA, it is hard to determine whether X3 will be positively contributing to salary or negatively

ii. inccorect, same reason as above, if GPA is larger than 3.5, then male earns more than female, vice versa

iii. correct, if GPA is higher than .53, then male earns more than female

iv. inccorect, the GPA would need to be below 3.5 for female to earn more than male

(b)

50+20*4+0.07*110+35*1+0.01*4*110-10*4*1=137.1

(c)

False, small coefficient could be attributed to large variable values, unless statistical test is conducted and p-value is calculated, it is hard to prove whether a term is significantly/not significantly contributing to salary.

### Question 4 ###

(a)

Training RSS would be lower for cubic regression compared to the linear regression, although the true relation is linear, cubic regression provides more rooms for fitting the trainning set data, since the sole purpose of fitting is to reduce training RSS, with more terms added to the equation, smaller RSS would be expected.

(b)

Since the true relationship is linear, the test RSS would tend to reveal that cubic regression creates an overfit in the model with generally larger RSS than linear regression.

(c)

As we illustrated in question (a), with more terms added to the fitting model for cubic regression, the training RSS would be at least the same or lower compared to linear regression

(d)

Since cubic regression would help better fit the nonlinearity of the data set, it is likely that test RSS is better for cubic regression compared to linear regression

### Question 5 ###

$$ \hat{y}{i} = x{i} \times \frac{\sum_{i'=1}^{n}\left ( x_{i'} y_{i'} \right )}{\sum_{j=1}^{n} x_{j}^{2}} $$

$$ \hat{y}{i} = \sum_{i'=1}^{n} \frac{\left ( x_{i'} y_{i'} \right ) \times x_{i}}{\sum_{j=1}^{n} x_{j}^{2}} $$

$$ \hat{y}{i} = \sum_{i'=1}^{n} \left ( \frac{ x_{i} x_{i'} } { \sum_{j=1}^{n} x_{j}^{2} } \times y_{i'} \right ) $$

$$ a_{i'} = \frac{ x_{i} x_{i'} } { \sum_{j=1}^{n} x_{j}^{2} } $$

### Question 6 ###

According to the equation 3.4,

$$ \hat{\beta}_{1} = \frac{\sum_{i=1}^{n} (x_i-\bar{x})(y_i-\bar{y})} {\sum_{i=1}^{n} (x_i-\bar{x})^2} $$

$$ \hat{\beta}_{0} = \bar{y} - \hat{\beta}_{1}\bar{x} $$

beta 1 would equal to 0 when xi equal to x average. beta 0 would equal to y average. Hence the model would prove to be valid regardless.

### Question 7 ###

An later exercise ...

### Question 8 ###

(a)

```{r}
require(ISLR)
data(Auto)
lm.fit <- lm(mpg~horsepower, data=Auto)
summary(lm.fit)
```

i. Yes, according to p-value, there is a significant relationship between the predictor and the response.
ii. The relationship is really strong, since p-value is close to 0
iii. The coefficient estimate suggest a positive relationship

iv.

```{r}
predict(lm.fit, data.frame(horsepower=c(98)))
predict(lm.fit, data.frame(horsepower=c(98)), interval = "confidence")
predict(lm.fit, data.frame(horsepower=c(98)), interval = "prediction")
```

(b)

```{r}
plot(Auto$horsepower,Auto$mpg)
abline(lm.fit, lwd=3)
```

(c)

```{r}
par(mfrow=c(2,2))
plot(lm.fit)
```

According to Residuals vs. Fitted plot, the residuals seem to be non-linear

### Question 9 ###

(a)

```{r}
require(ISLR)
data(Auto)
pairs(Auto)
```

(b)

```{r}
cor(subset(Auto, select=-c(name)))
```

(c)

```{r}
lm.fit <- lm(mpg~.-name,data=Auto)
summary(lm.fit)
```

i. Yes, overall p-value is close to 0 indicating there is a strong relationship between the predictors and the response

ii. displacement, weight, year and origin were displaying a statistically significant relationship to the response.

iii. it suggest for every year of increment of the car which are produced, the mpg would increase for about 0.75.

(d)

```{r}
par(mfrow=c(2,2))
plot(lm.fit)
```

The residual plot suggested some outliers on top right corner of the chart, the leverage plot shows observation 14 has an outstanding leverage compared to rest of the data sets.

(e)

```{r}
lm.fit <- lm(mpg~.-name-displacement-acceleration-cylinders+weight*year+horsepower*origin,data=Auto)
summary(lm.fit)
```

Tried the new model with a more spread and linear distribution of residuals

(f)

```{r}
lm.fit <- lm(mpg~.-name-displacement-acceleration-cylinders+weight*year+log(horsepower)+I(year^2),data=Auto)
summary(lm.fit)
```

log horsepower and year^2 does provide more explanatory power to the data set, resulting in higher adjusted R-Squared as well.

### Question 10 ###

(a)

```{r}
data(Carseats)
lm.fit <- lm(Sales~Price+Urban+US,data=Carseats)
summary(lm.fit)
```

(b)

Price is negatively correlated with Sales, if the store is in urban areas, that would result in less sales, if the store is in US, it will result in more sales.

(c)

$$ Sales_{i} = 13.043 - 0.054Price_{i} - 0.0219Urban_{i} + 1.201US_{i} $$

(d)

For Urban predictors, I can reject it  since p-value does not suggest a significant relationship

(e)

```{r}
lm.fit <- lm(Sales~Price+US,data=Carseats)
summary(lm.fit)
```

(f)

The adjusted R-Square is slightly higher in later case, but both of them are pretty low

(g)

```{r}
confint(lm.fit)
```

(h)

```{r}
par(mfrow=c(2,2))
plot(lm.fit)
```

There is evidence of high leverage for one observation, no evidence of clear outliers

### Question 11 ###

```{r}
set.seed(1)
x <- rnorm(100)
y <- 2*x+rnorm(100)
```

(a)

```{r}
lm.fit <- lm(y~x+0)
summary(lm.fit)
```

The coefficient estimate $$ \hat{\beta} $$ is 1.9939. The standard error of the estimate is 0.1065. Given significant large t value coupled with close to 0 p-value we can reject null hypothesis and conclude that there is a significant relationship between y and x.

(b)

```{r}
lm.fit <- lm(x~y+0)
summary(lm.fit)
```

The coefficient estimate $$ \hat{\beta} $$ is 0.39111. The standard error of the estimate is 0.02089. Given significant large t value coupled with close to 0 p-value we can reject null hypothesis and conclude that there is a significant relationship between x and y.

(c)

inversely correlated

(d)

...

(e)

As the formula suggested, by exchanging x and y in the formulat does not really change the outcome of the result since it's all product terms, hence the t-statistic for both regression should be the same.

(f)

```{r}
lm1.fit <- lm(y~x)
lm2.fit <- lm(x~y)
summary(lm1.fit)
summary(lm2.fit)
```

t-statistics for both are very close

### Question 12 ###

(a)

When xi equal to yi, then the estimator will be the same, and sd is small enough and not affecting prediction of estimator

(b)

```{r}
set.seed(1)
x <- rnorm(100)
y <- 2*x+rnorm(100)
```

In this case coefficient would be different between X onto Y and Y onto X.

```{r}
lm1.fit <- lm(y~x+0)
lm2.fit <- lm(x~y+0)
summary(lm1.fit)
summary(lm2.fit)
```

(c)

```{r}
set.seed(1)
x <- rnorm(100)
y <- x + rnorm(100, mean = 0, sd = 0.001)
```

In this case, they will be the same

```{r}
lm1.fit <- lm(y~x+0)
lm2.fit <- lm(x~y+0)
summary(lm1.fit)
summary(lm2.fit)
```

### Question 13 ###

(a)

```{r}
set.seed(1)
x <- rnorm(100, mean = 0, sd = 1)
```


(b)

```{r}
eps <- rnorm(100, mean = 0, sd = 0.25)
```

(c)

```{r}
y <- -1 + 0.5*x + eps
length(y)
```

vector length is 100.

intercept term is around -1 and coefficient for x is around 0.5.

(d)

```{r}
plot(x,y)
```

I can observe a relatively strong linear relationship

(e)

```{r}
lm.fit <- lm(y~x)
summary(lm.fit)
```

modelled intercept term is -1.00942, modelled coefficient term is 0.49973.

(f)

```{r}
plot(x,y)
abline(-1,0.5, col = "blue")
abline(lm.fit, col = "red")
legend(x = c(0.2,7),
       y = c(-1.8,-2.3),
       legend = c("population", "model fit"),
       col = c("blue","red"), lwd = 2)
```

(g)

```{r}
lm.fit.poly <- lm(y~x+I(x^2))
summary(lm.fit.poly)
```

No, the quadratic term does not prove to be significantly correlated with result term, hence it does not increase the model fit.

(h)

```{r}
eps <- rnorm(100, mean = 0, sd = 0.5)
y <- -1 + 0.5*x + eps
lm.fit.more.noisy <- lm(y~x)
summary(lm.fit.more.noisy)
```

(i)

```{r}
eps <- rnorm(100, mean = 0, sd = 0.1)
y <- -1 + 0.5*x + eps
lm.fit.less.noisy <- lm(y~x)
summary(lm.fit.less.noisy)
```

(j)

```{r}
confint(lm.fit)
confint(lm.fit.more.noisy)
confint(lm.fit.less.noisy)
```

The confidence interval is larger when the data set is noisier and vice versa

### Question 14 ###

(a)

```{r}
set.seed(1)
x1 <- runif(100)
x2 <- 0.5*x1 + rnorm(100)/10
y <- 2 + 2*x1 + 0.3*x2 + rnorm(100)
```

population regression is $$ y = {\beta}_{0} + {\beta}_{1}x_{1} + {\beta}_{2}x_{2} + {\epsilon} $$

The coefficient are $$ {\beta}_{0} = 2 $$, $$ {\beta}_{1} = 2 $$ and $$ {\beta}_{2} = 0.3 $$

(b)

```{r}
plot(x1, x2)
```

There is a relatively strong linear relationship between the two variables.

(c)

```{r}
lm.fit <- lm(y~x1+x2)
summary(lm.fit)
```

Intercept is mostly close to population intercept, but both x1 and x2 are slightly far away from true x1 and x2.

Under 95% confidence interval we will reject hypothesis, but we won't reject hypothesis on beta2

(d)

```{r}
lm.fit <- lm(y~x1)
summary(lm.fit)
```

Yes, we can reject the null hypothesis, since the relationship is significant

(e)

```{r}
lm.fit <- lm(y~x2)
summary(lm.fit)
```

Yes, we can reject the null hypothesis, since the relationship is signficant

(f)

No, they don't. As far as we can tell based on the real relationship, x1 can mostly explained the movements in x2. Hence when both x1 and x2 are presented as coefficient, due to collinearility, only one variable is actually needed for the model resulting us rejecting x2.

In other scenarios, only one variable is used, hence we cannot reject any of them.

(g)

```{r}
x1 <- c(x1, 0.1)
x2 <- c(x2, 0.8)
y <- c(y, 6)
```

```{r}
lm.fit <- lm(y~x1+x2)
par(mfrow=c(2,2))
plot(lm.fit)
```

Clearly the newly added item 101 has a high leverage in the data set compared with other data

### Question 15 ###

(a)

```{r}
library(MASS)
names(Boston)[-1]
```

```{r}
lmp <- function (modelobject) {
    if (class(modelobject) != "lm") 
      stop("Not an object of class 'lm' ")
    f <- summary(modelobject)$fstatistic
    p <- pf(f[1],f[2],f[3],lower.tail=F)
    attributes(p) <- NULL
    return(p)
}

results <- combn(names(Boston), 2, 
                 function(x) { lmp(lm(Boston[, x])) }, 
                 simplify = FALSE)

vars <- combn(names(Boston), 2)
names(results) <- paste(vars[1,],vars[2,],sep="~")
results[1:13]
```

The rad variable has a very significant correlation with per capita crime rate by town. At the same time, it produced higher R square as well.

(b)

```{r}
lm.fit.multi <- lm(crim~., data=Boston)
summary(lm.fit.multi)
```

The above model is developed using backwardation method, and end up with only zn, dis, rad and medv as the variables. Under this scenario, we can reject null hypothesis.

(c)

```{r}
results <- combn(names(Boston), 2, 
                 function(x) { coefficients(lm(Boston[, x])) }, 
                 simplify = FALSE)
(coef.uni <- unlist(results)[seq(2,26,2)])
```

```{r}
(coef.multi <- coefficients(lm.fit.multi)[-1])
```

```{r}
plot(coef.uni, coef.multi)
```

beta coefficient tend to be very different between multivariate regression and single variable regression.

(d)

```{r}
lm.fit.poly <- lm(crim~poly(zn,3), data = Boston)
summary(lm.fit.poly)
```

The significance goes all the way up to square term then there is no evidence to prove further polynomial term to be significant for zn term