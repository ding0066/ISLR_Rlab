### Question 1 ###

Based on Table 3.4, we can conclude that both TV and radio is has significant impact on sales performance. While newspaper does not prove to have a significant impact on sales.

### Question 2 ###

KNN regression averages the closest observations to estimate predict, KNN classifier assigns classification group based on majority of closest observations.

### Question 3 ###

(a)

i. inccorect, since there is an interaction term between gender and GPA, it is hard to determine whether X3 will be positively contributing to salary or negatively

ii. inccorect, same reason as above, if GPA is larger than 3.5, then male earns more than female, vice versa

iii. correct, if GPA is higher than .53, then male earns more than female

iv. inccorect, the GPA would need to be below 3.5 for female to earn more than male

(b)

50+20*4+0.07*110+35*1+0.01*4*110-10*4*1=137.1

(c)

False, small coefficient could be attributed to large variable values, unless statistical test is conducted and p-value is calculated, it is hard to prove whether a term is significantly/not significantly contributing to salary.

### Question 4 ###

(a)

Training RSS would be lower for cubic regression compared to the linear regression, although the true relation is linear, cubic regression provides more rooms for fitting the trainning set data, since the sole purpose of fitting is to reduce training RSS, with more terms added to the equation, smaller RSS would be expected.

(b)

Since the true relationship is linear, the test RSS would tend to reveal that cubic regression creates an overfit in the model with generally larger RSS than linear regression.

(c)

As we illustrated in question (a), with more terms added to the fitting model for cubic regression, the training RSS would be at least the same or lower compared to linear regression

(d)

Since cubic regression would help better fit the nonlinearity of the data set, it is likely that test RSS is better for cubic regression compared to linear regression

### Question 5 ###

$$ \hat{y}{i} = x{i} \times \frac{\sum_{i'=1}^{n}\left ( x_{i'} y_{i'} \right )}{\sum_{j=1}^{n} x_{j}^{2}} $$

$$ \hat{y}{i} = \sum_{i'=1}^{n} \frac{\left ( x_{i'} y_{i'} \right ) \times x_{i}}{\sum_{j=1}^{n} x_{j}^{2}} $$

$$ \hat{y}{i} = \sum_{i'=1}^{n} \left ( \frac{ x_{i} x_{i'} } { \sum_{j=1}^{n} x_{j}^{2} } \times y_{i'} \right ) $$

$$ a_{i'} = \frac{ x_{i} x_{i'} } { \sum_{j=1}^{n} x_{j}^{2} } $$

### Question 6 ###

According to the equation 3.4,

$$ \hat{\beta}_{1} = \frac{\sum_{i=1}^{n} (x_i-\bar{x})(y_i-\bar{y})} {\sum_{i=1}^{n} (x_i-\bar{x})^2} $$

$$ \hat{\beta}_{0} = \bar{y} - \hat{\beta}_{1}\bar{x} $$

beta 1 would equal to 0 when xi equal to x average. beta 0 would equal to y average. Hence the model would prove to be valid regardless.

### Question 7 ###

An later exercise ...

### Question 8 ###

(a)

```{r}
require(ISLR)
data(Auto)
lm.fit <- lm(mpg~horsepower, data=Auto)
summary(lm.fit)
```

i. Yes, according to p-value, there is a significant relationship between the predictor and the response.
ii. The relationship is really strong, since p-value is close to 0
iii. The coefficient estimate suggest a positive relationship

iv.

```{r}
predict(lm.fit, data.frame(horsepower=c(98)))
predict(lm.fit, data.frame(horsepower=c(98)), interval = "confidence")
predict(lm.fit, data.frame(horsepower=c(98)), interval = "prediction")
```

(b)

```{r}
plot(Auto$horsepower,Auto$mpg)
abline(lm.fit, lwd=3)
```

(c)

```{r}
par(mfrow=c(2,2))
plot(lm.fit)
```

According to Residuals vs. Fitted plot, the residuals seem to be non-linear

### Question 9 ###

(a)

```{r}
require(ISLR)
data(Auto)
pairs(Auto)
```

(b)

```{r}

```


